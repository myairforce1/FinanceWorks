{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 资产收益预估统计模型的总体构造\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用机器学习模型进行资产回报的预估，建立如下的加法模型\n",
    "$$\n",
    "r_{i, t+1} = E_t(r_{i, t+1}) + \\epsilon_{i, t+1} \\tag{1}\n",
    "$$\n",
    "其中$$\n",
    "E_t(r_{i, t+1}) = g^*(z_{i, t+1}) \\tag{2}\n",
    "$$\n",
    "\n",
    "$i = 1, 2, ... N_t$为股票id， $t = 1, 2,...,T$为月份序号。\n",
    "即第i个股票第t+1期的收益为期望收益 + 误差项。\n",
    "\n",
    "模型中的期望函数g是因子$z_{i, t+1}$的函数， 模型对不同的股票i和不同的时间t，估计同一个期望函数g，并保证g在样本集外有最大的预测能力。\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 样本集合划分和验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 训练集\n",
    "2. 验证集\n",
    "3. 测试集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 几种主要的机器学习方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 简单线性模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性最小二乘法。对每一个因子引入权重$\\theta_{i, t}$, $\\theta = (\\theta_{i, t})$。\n",
    "\n",
    "$$\n",
    "g(z_{i, t};\\theta) = z^{T}_{i, t} \\theta \\tag{3}\n",
    "$$\n",
    "\n",
    "目标函数：\n",
    "\n",
    "$$\n",
    "   L(\\theta) = \\frac{1}{NT} \\sum_1^{N}\\sum_1^T (r_{i, t+1} - g(z_{i, t}; \\theta))^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 增强鲁棒性的线性模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "带权重的最小二乘:\n",
    "$$\n",
    "L(\\theta) = \\frac{1}{NT} \\sum_1^{N}\\sum_1^T w_{i, t}(r_{i, t+1} - g(z_{i, t}; \\theta))^2  \\tag{5}\n",
    "$$\n",
    "\n",
    "权重项$w_{i, t}$, 可以反比于某一个月的股票种类数。这样月份t对于损失的贡献是一样的。\n",
    "\n",
    "也可以正比于证券i在时间t的市值。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用Huber loss：\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\frac{1}{NT} \\sum_1^{N}\\sum_1^T H((r_{i, t+1} - g(z_{i, t}; \\theta)); \\xi) \\tag{6}\n",
    "$$\n",
    "\n",
    "where:\n",
    "$$\n",
    "H(X, \\xi) = \\begin{cases}\n",
    "x^2& if  |x| <= \\xi\\\\\n",
    " 2\\xi |x| - \\xi^2&   if |x| > \\xi\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即Huber Loss在差异值小于$\\xi$时，用的是平方误差，大于$\\xi$时用的线性误差。HuberLoss针对outlier（特异点）具有较好的稳定性，不会因为特异点的存在使得loss function急剧增大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](pics/huber.JPG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 带惩罚项的线性模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "惩罚因子对模型的复杂性进行惩罚，降低过拟合的可能性。\n",
    "\n",
    "$$\n",
    "L(\\theta; .) = L(\\theta) + \\phi(\\theta)  \\tag{7}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic Net惩罚模型：\n",
    "$$\n",
    "\\phi(\\theta, \\lambda, \\rho) = \\lambda (1 - \\rho)\\sum_{j=1}^{P}|\\theta_j|  + \\frac{1}{2}\\lambda\\rho\\sum_{j=1}^P\\theta_j^2 \\tag{8}\n",
    "$$\n",
    "\n",
    "Elastic Net惩罚模型是说，综合使用了L1(Lasso)和L2(Ridge)的惩罚方式，当$\\rho=0$时，就是Lasso回归， 当$\\rho=1$时就是Ridge回归。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 降维方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 广义线性模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 集成学习方法: Boosting Regression Trees和Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.1 回归树和分类树\n",
    "\n",
    "非参数模型。寻找特征的之间组合，实现对样本空间的划分。\n",
    "\n",
    "\n",
    "对于分类树，有三种算法： ID3, C4.5, CART。\n",
    "\n",
    "对于回归树：CART。\n",
    "\n",
    "\n",
    "ID3:  \n",
    "\n",
    "\n",
    "C4.5:\n",
    "\n",
    "\n",
    "CART分类:\n",
    "\n",
    "\n",
    "CART回归：\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.2 Boosting\n",
    "\n",
    "Boosting框架： 多个较弱的分类器进行组合，加法模型。本次迭代的tree模型结果加到之前的树模型中，使得残差最小化。\n",
    "\n",
    "$$\n",
    "g(z) = \\sum_{i=1}^{k} f_i(z)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.2.1 GBDT\n",
    "\n",
    "Gradient Based Decision Tree。 在GBDT中，使用的树都是cart回归树。\n",
    "例如有若干个样本点$(x_1, y_1), (x_2, y_2).....(x_n, y_n)$。 希望构造一个拟合函数F(x)，使得经验风险Loss(y, F(x))最小化。\n",
    "\n",
    "如果我们已经学习到一个拟合函数$f_1(x)$, 这时我们得到了每个样本点的拟合结果: $$(x_1, f_1(x_1)), (x_2, f_1(x_2)), ...., (x_n, f_1(x_n))$$, 与此同时可以得到每个样本点的残差$$r_i = y_i - f_1(x_i)$$, 这时我们希望学习一个新的函数f_2(x), 它可以最优拟合这个残差。这是一个新的拟合问题，即boosting算法的第二次迭代。得到$f_2(x)$。为避免过拟合，引入一个学习步长$\\lambda$, 第二次残差为 $$ y_i - f_1(x_i) - \\lambda f_2(x_i)$$\n",
    "\n",
    "\n",
    "如此迭代直到迭代的最大步骤数。\n",
    "\n",
    "\n",
    "这里我们可以假设每次拟合的损失函数是平方差损失函数(MSE)，$$ loss_k = (y_i - f_k(x_i))^2$$, 这个损失函数的一阶导就是上面提到的残差。拟合残差就是拟合损失函数的梯度。使用不同的损失函数就拟合不同的梯度，故名Gradient Based Decision Tree。\n",
    "\n",
    "在Boosting框架中，可以选取大量的树(如>100颗), 但是每棵树非常简单，这样可以有效防止过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.2.2 Xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.7 神经网络"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "tfenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
